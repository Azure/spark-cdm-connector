{"cells":[{"cell_type":"code","source":["appid = \"<appId>\"\nappkey = \"<appKey>\"\ntenantid = \"<tenantId>\"\n\ncontainer = \"<demoContainerName>\"\nstorageAccountName = \"<storageAccount>.dfs.core.windows.net\""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["# Implicit write case\nfrom pyspark.sql.types import *\nfrom pyspark.sql import functions, Row\nfrom decimal import Decimal\nfrom datetime import datetime\n\n# Write a CDM entity with Parquet data files, entity definition is derived from the dataframe schema\nd = datetime.strptime(\"2015-03-31\", '%Y-%m-%d')\nts = datetime.now()\ndata = [\n  [\"a\", 1, True, 12.34, 6, d, ts, Decimal(1.4337879), Decimal(999.00), Decimal(18.8)],\n  [\"b\", 1, True, 12.34, 6, d, ts, Decimal(1.4337879), Decimal(999.00), Decimal(18.8)]\n]\n\nschema = (StructType()\n  .add(StructField(\"name\", StringType(), True))\n  .add(StructField(\"id\", IntegerType(), True))\n  .add(StructField(\"flag\", BooleanType(), True))\n  .add(StructField(\"salary\", DoubleType(), True))\n  .add(StructField(\"phone\", LongType(), True))\n  .add(StructField(\"dob\", DateType(), True))\n  .add(StructField(\"time\", TimestampType(), True))\n  .add(StructField(\"decimal1\", DecimalType(15, 3), True))\n  .add(StructField(\"decimal2\", DecimalType(38, 7), True))\n  .add(StructField(\"decimal3\", DecimalType(5, 2), True))\n)\n\ndf = spark.createDataFrame(spark.sparkContext.parallelize(data), schema)\n\n# Creates the CDM manifest and adds the entity to it with gzip'd parquet partitions\n# with both physical and logical entity definitions \n(df.write.format(\"com.microsoft.cdm\")\n  .option(\"storage\", storageAccountName)\n  .option(\"container\", container)\n  .option(\"manifest\", \"/implicitTest/default.manifest.cdm.json\")\n  .option(\"entity\", \"TestEntity\")\n  .option(\"format\", \"parquet\")\n  .option(\"compression\", \"gzip\")\n  .option(\"appId\", appid)\n  .option(\"appKey\", appkey)\n  .option(\"tenantId\", tenantid)\n  .save())\n\n# Append the same dataframe content to the entity in the default CSV format\n(df.write.format(\"com.microsoft.cdm\")\n  .option(\"storage\", storageAccountName)\n  .option(\"container\", container)\n  .option(\"manifest\", \"/implicitTest/default.manifest.cdm.json\")\n  .option(\"entity\", \"TestEntity1\")\n  .option(\"appId\", appid)\n  .option(\"appKey\", appkey)\n  .option(\"tenantId\", tenantid)\n  .mode(\"append\")\n  .save())\n\nreadDf = (spark.read.format(\"com.microsoft.cdm\")\n  .option(\"storage\", storageAccountName)\n  .option(\"container\", container)\n  .option(\"manifest\", \"/implicitTest/default.manifest.cdm.json\")\n  .option(\"entity\", \"TestEntity\")\n  .option(\"appId\", appid)\n  .option(\"appKey\", appkey)\n  .option(\"tenantId\", tenantid)\n  .load())\n\nreadDf.select(\"*\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+---+----+------+-----+----------+-------------------+--------+-----------+--------+\nname| id|flag|salary|phone|       dob|               time|decimal1|   decimal2|decimal3|\n+----+---+----+------+-----+----------+-------------------+--------+-----------+--------+\n   a|  1|true| 12.34|    6|2015-03-31|2020-05-20 10:13:04|   1.434|999.0000000|   18.80|\n   b|  1|true| 12.34|    6|2015-03-31|2020-05-20 10:13:04|   1.434|999.0000000|   18.80|\n+----+---+----+------+-----+----------+-------------------+--------+-----------+--------+\n\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["# Explicit write, creating an entity in a CDM folder based on a pre-defined model \n\n# Case 1: Using an entity definition defined in the CDM Github repo\n\ndata = [\n  [\"1\", \"2\", \"3\", 4],\n  [\"4\", \"5\", \"6\", 8],\n  [\"7\", \"8\", \"9\", 4],\n  [\"10\", \"11\", \"12\", 8],\n  [\"13\", \"14\", \"15\", 4]\n]\n\nschema = (StructType()\n  .add(StructField(\"teamMembershipId\", StringType(), True))\n  .add(StructField(\"systemUserId\", StringType(), True))\n  .add(StructField(\"teamId\", StringType(), True))\n  .add(StructField(\"versionNumber\", LongType(), True))\n)\n\ndf = spark.createDataFrame(spark.sparkContext.parallelize(data,1), schema)\n          \n(df.write.format(\"com.microsoft.cdm\")\n  .option(\"storage\", storageAccountName)\n  .option(\"container\", container)\n  .option(\"manifest\", \"explicitTest/root.manifest.cdm.json\")\n  .option(\"entity\", \"TeamMembership\")\n  .option(\"entityDefinition\", \"core/applicationCommon/TeamMembership.cdm.json/TeamMembership\")\n  .option(\"useCdmGithubModelRoot\", True)  # sets the model root to the CDM GitHub schema documents folder\n  .option(\"useSubManifest\", True)\n  .option(\"appId\", appid)\n  .option(\"appKey\", appkey)\n  .option(\"tenantId\", tenantid)\n  .mode(\"overwrite\")\n  .save())\n\nreadDf = (spark.read.format(\"com.microsoft.cdm\")\n  .option(\"storage\", storageAccountName)\n  .option(\"container\", container)\n  .option(\"manifest\", \"/explicitTest/root.manifest.cdm.json\")\n  .option(\"entity\", \"TeamMembership\")\n  .option(\"useCDMGithub\", True) # sets the modelroot alias to the CDM GitHub schema documents folder\n  .option(\"appId\", appid)\n  .option(\"appKey\", appkey)\n  .option(\"tenantId\", tenantid)\n  .load())\n          \nreadDf.select(\"*\").show()"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# Explicit write, creating an entity in a CDM folder based on a pre-defined model \n\n# Case 2: Using an entity definition defined in a CDM model stored in ADLS\n\n# UPLOAD CDM FILES FIRST\n# To run this example, first create a /Models/Contacts folder to your demo container in ADLS gen2,\n# then upload the provided Contacts.manifest.cdm.json, Person.cdm.json, Entity.cdm.json files\n\nbirthdate = datetime.strptime(\"1991-03-31\", '%Y-%m-%d')\nnow = datetime.now()\ndata2 = [\n  [1,now,\"Donna\",\"Carreras\",birthdate],\n  [2,now,\"Keith\",\"Harris\",birthdate],\n  [2,now,\"Carla\",\"McGee\",birthdate]\n]\n\nschema2 = (StructType()\n  .add(StructField(\"identifier\", IntegerType()))\n  .add(StructField(\"createdTime\", TimestampType()))\n  .add(StructField(\"firstName\", StringType()))\n  .add(StructField(\"lastName\", StringType()))\n  .add(StructField(\"birthDate\", DateType())))\n\n# Create the dataframe that matches the CDM definition of the entity, Person\ndf2 = spark.createDataFrame(spark.sparkContext.parallelize(data2, 1), schema2)\n(df2.write.format(\"com.microsoft.cdm\")\n  .option(\"storage\", storageAccountName)\n  .option(\"container\", container)\n  .option(\"manifest\", \"/Data/Contacts/root.manifest.cdm.json\")\n  .option(\"entity\", \"Person\")\n  .option(\"entityDefinitionContainer\", container)\n  .option(\"entityDefinitionModelRoot\", \"Models\") \n  .option(\"entityDefinition\", \"/Contacts/Person.cdm.json/Person\")   \n  .option(\"appId\", appid).option(\"appKey\", appkey).option(\"tenantId\", tenantid)\n  .mode(\"overwrite\")\n  .save())\n\nreadDf2 = (spark.read.format(\"com.microsoft.cdm\")\n  .option(\"storage\", storageAccountName)\n  .option(\"container\", container)\n  .option(\"manifest\", \"/Data/Contacts/root.manifest.cdm.json\")\n  .option(\"entity\", \"Person\")\n  .option(\"entityDefinitionContainer\", container)\n  .option(\"entityDefinitionModelRoot\", \"Models\")\n  .option(\"appId\", appid).option(\"appKey\", appkey).option(\"tenantId\", tenantid)\n  .load())\n\nreadDf2.select(\"*\").show()"],"metadata":{},"outputs":[],"execution_count":4}],"metadata":{"name":"SparkCDMDemoPython","notebookId":2456676788671898},"nbformat":4,"nbformat_minor":0}
